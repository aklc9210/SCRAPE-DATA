import re
from typing import List, Dict
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from datetime import datetime
from pymongo import UpdateOne

# ===== TRANSLATION SETUP =====
tokenizer_vi2en = AutoTokenizer.from_pretrained(
    "vinai/vinai-translate-vi2en-v2",
    use_fast=False,
    src_lang="vi_VN",
    tgt_lang="en_XX"
)
model_vi2en = AutoModelForSeq2SeqLM.from_pretrained("vinai/vinai-translate-vi2en-v2")

# ===== CATEGORIES MAPPING =====
VALID_TITLES = {
    # Th·ªãt, c√°, tr·ª©ng
    "Th·ªãt heo", "Th·ªãt b√≤", "Th·ªãt g√†, v·ªãt, chim", "Th·ªãt s∆° ch·∫ø", "Tr·ª©ng g√†, v·ªãt, c√∫t",
    "C√°, h·∫£i s·∫£n, kh√¥", "C√° h·ªôp", "L·∫°p x∆∞·ªüng", "X√∫c x√≠ch", "Heo, b√≤, pate h·ªôp",
    "Ch·∫£ gi√≤, ch·∫£ ram", "Ch·∫£ l·ª•a, th·ªãt ngu·ªôi", "X√∫c x√≠ch, l·∫°p x∆∞·ªüng t∆∞∆°i",
    "C√° vi√™n, b√≤ vi√™n", "Th·ªãt, c√° ƒë√¥ng l·∫°nh",

    # Rau, c·ªß, qu·∫£, n·∫•m
    "Tr√°i c√¢y", "Rau l√°", "C·ªß, qu·∫£", "N·∫•m c√°c lo·∫°i", "Rau, c·ªß l√†m s·∫µn",
    "Rau c·ªß ƒë√¥ng l·∫°nh",

    # ƒê·ªì ƒÉn chay
    "ƒê·ªì chay ƒÉn li·ªÅn", "ƒê·∫≠u h≈©, ƒë·ªì chay kh√°c", "ƒê·∫≠u h≈©, t√†u h≈©",

    # Ng≈© c·ªëc, tinh b·ªôt
    "Ng≈© c·ªëc", "Ng≈© c·ªëc, y·∫øn m·∫°ch", "G·∫°o c√°c lo·∫°i", "B·ªôt c√°c lo·∫°i",
    "ƒê·∫≠u, n·∫•m, ƒë·ªì kh√¥",

    # M√¨, b√∫n, ph·ªü, ch√°o
    "M√¨ ƒÉn li·ªÅn", "Ph·ªü, b√∫n ƒÉn li·ªÅn", "H·ªß ti·∫øu, mi·∫øn", "Mi·∫øn, h·ªß ti·∫øu, ph·ªü kh√¥",
    "M√¨ √ù, m√¨ tr·ª©ng", "Ch√°o g√≥i, ch√°o t∆∞∆°i", "B√∫n c√°c lo·∫°i", "Nui c√°c lo·∫°i",
    "B√°nh tr√°ng c√°c lo·∫°i", "B√°nh ph·ªìng, b√°nh ƒëa", "B√°nh g·∫°o H√†n Qu·ªëc",

    # Gia v·ªã, ph·ª• gia, d·∫ßu
    "N∆∞·ªõc m·∫Øm", "N∆∞·ªõc t∆∞∆°ng", "T∆∞∆°ng, chao c√°c lo·∫°i", "T∆∞∆°ng ·ªõt - ƒëen, mayonnaise",
    "D·∫ßu ƒÉn", "D·∫ßu h√†o, gi·∫•m, b∆°", "Gia v·ªã n√™m s·∫µn", "Mu·ªëi",
    "H·∫°t n√™m, b·ªôt ng·ªçt, b·ªôt canh", "Ti√™u, sa t·∫ø, ·ªõt b·ªôt", "B·ªôt ngh·ªá, t·ªèi, h·ªìi, qu·∫ø,...",
    "N∆∞·ªõc ch·∫•m, m·∫Øm", "M·∫≠t ong, b·ªôt ngh·ªá",

    # S·ªØa & c√°c s·∫£n ph·∫©m t·ª´ s·ªØa
    "S·ªØa t∆∞∆°i", "S·ªØa ƒë·∫∑c", "S·ªØa pha s·∫µn", "S·ªØa h·∫°t, s·ªØa ƒë·∫≠u", "S·ªØa ca cao, l√∫a m·∫°ch",
    "S·ªØa tr√°i c√¢y, tr√† s·ªØa", "S·ªØa chua ƒÉn", "S·ªØa chua u·ªëng li·ªÅn", "B∆° s·ªØa, ph√¥ mai",

    # ƒê·ªì u·ªëng
    "Bia, n∆∞·ªõc c√≥ c·ªìn", "R∆∞·ª£u", "N∆∞·ªõc tr√†", "N∆∞·ªõc ng·ªçt", "N∆∞·ªõc √©p tr√°i c√¢y",
    "N∆∞·ªõc y·∫øn", "N∆∞·ªõc tƒÉng l·ª±c, b√π kho√°ng", "N∆∞·ªõc su·ªëi", "C√† ph√™ ho√† tan",
    "C√† ph√™ pha phin", "C√† ph√™ lon", "Tr√† kh√¥, t√∫i l·ªçc",

    # B√°nh k·∫πo, snack
    "B√°nh t∆∞∆°i, Sandwich", "B√°nh b√¥ng lan", "B√°nh quy", "B√°nh snack, rong bi·ªÉn",
    "B√°nh Chocopie", "B√°nh g·∫°o", "B√°nh qu·∫ø", "B√°nh que", "B√°nh x·ªëp",
    "K·∫πo c·ª©ng", "K·∫πo d·∫ªo, k·∫πo marshmallow", "K·∫πo singum", "Socola",
    "Tr√°i c√¢y s·∫•y", "H·∫°t kh√¥", "Rong bi·ªÉn c√°c lo·∫°i", "Rau c√¢u, th·∫°ch d·ª´a",
    "M·ª©t tr√°i c√¢y", "C∆°m ch√°y, b√°nh tr√°ng",

    # M√≥n ƒÉn ch·∫ø bi·∫øn s·∫µn, ƒë√¥ng l·∫°nh
    "L√†m s·∫µn, ƒÉn li·ªÅn", "S∆° ch·∫ø, t·∫©m ∆∞·ªõp", "N∆∞·ªõc l·∫©u, vi√™n th·∫£ l·∫©u",
    "Kim chi, ƒë·ªì chua", "Mandu, h√° c·∫£o, s·ªßi c·∫£o", "B√°nh bao, b√°nh m√¨, pizza",
    "Kem c√¢y, kem h·ªôp", "B√°nh flan, th·∫°ch, ch√®", "Tr√°i c√¢y h·ªôp, siro",

    "C√° m·∫Øm, d∆∞a m·∫Øm", "ƒê∆∞·ªùng", "N∆∞·ªõc c·ªët d·ª´a lon", "S·ªØa chua u·ªëng", "Kh√¥ ch·∫ø bi·∫øn s·∫µn"
}

CATEGORIES_MAPPING = {
    # Th·ªãt, c√°, tr·ª©ng
    "Th·ªãt heo": "Fresh Meat",
    "Th·ªãt b√≤": "Fresh Meat", 
    "Th·ªãt g√†, v·ªãt, chim": "Fresh Meat",
    "Th·ªãt s∆° ch·∫ø": "Fresh Meat",
    "Tr·ª©ng g√†, v·ªãt, c√∫t": "Fresh Meat",
    "C√°, h·∫£i s·∫£n, kh√¥": "Seafood & Fish Balls",
    "C√° h·ªôp": "Instant Foods",
    "L·∫°p x∆∞·ªüng": "Cold Cuts: Sausages & Ham",
    "X√∫c x√≠ch": "Cold Cuts: Sausages & Ham",
    "Heo, b√≤, pate h·ªôp": "Instant Foods",
    "Ch·∫£ gi√≤, ch·∫£ ram": "Instant Foods",
    "Ch·∫£ l·ª•a, th·ªãt ngu·ªôi": "Cold Cuts: Sausages & Ham",
    "X√∫c x√≠ch, l·∫°p x∆∞·ªüng t∆∞∆°i": "Cold Cuts: Sausages & Ham",
    "C√° vi√™n, b√≤ vi√™n": "Instant Foods",
    "Th·ªãt, c√° ƒë√¥ng l·∫°nh": "Instant Foods",

    # Rau, c·ªß, qu·∫£, n·∫•m
    "Tr√°i c√¢y": "Fresh Fruits",
    "Rau l√°": "Vegetables",
    "C·ªß, qu·∫£": "Vegetables",
    "N·∫•m c√°c lo·∫°i": "Vegetables",
    "Rau, c·ªß l√†m s·∫µn": "Vegetables",
    "Rau c·ªß ƒë√¥ng l·∫°nh": "Vegetables",

    # ƒê·ªì ƒÉn chay
    "ƒê·ªì chay ƒÉn li·ªÅn": "Instant Foods",
    "ƒê·∫≠u h≈©, ƒë·ªì chay kh√°c": "Instant Foods",
    "ƒê·∫≠u h≈©, t√†u h≈©": "Instant Foods",

    # Ng≈© c·ªëc, tinh b·ªôt
    "Ng≈© c·ªëc": "Cereals & Grains",
    "Ng≈© c·ªëc, y·∫øn m·∫°ch": "Cereals & Grains",
    "G·∫°o c√°c lo·∫°i": "Grains & Staples",
    "B·ªôt c√°c lo·∫°i": "Grains & Staples",
    "ƒê·∫≠u, n·∫•m, ƒë·ªì kh√¥": "Grains & Staples",

    # M√¨, b√∫n, ph·ªü, ch√°o
    "M√¨ ƒÉn li·ªÅn": "Instant Foods",
    "Ph·ªü, b√∫n ƒÉn li·ªÅn": "Instant Foods",
    "H·ªß ti·∫øu, mi·∫øn": "Instant Foods",
    "Mi·∫øn, h·ªß ti·∫øu, ph·ªü kh√¥": "Instant Foods",
    "M√¨ √ù, m√¨ tr·ª©ng": "Instant Foods",
    "Ch√°o g√≥i, ch√°o t∆∞∆°i": "Instant Foods",
    "B√∫n c√°c lo·∫°i": "Instant Foods",
    "Nui c√°c lo·∫°i": "Instant Foods",
    "B√°nh tr√°ng c√°c lo·∫°i": "Instant Foods",
    "B√°nh ph·ªìng, b√°nh ƒëa": "Instant Foods",
    "B√°nh g·∫°o H√†n Qu·ªëc": "Cakes",

    # Gia v·ªã, ph·ª• gia, d·∫ßu
    "N∆∞·ªõc m·∫Øm": "Seasonings",
    "N∆∞·ªõc t∆∞∆°ng": "Seasonings",
    "T∆∞∆°ng, chao c√°c lo·∫°i": "Seasonings",
    "T∆∞∆°ng ·ªõt - ƒëen, mayonnaise": "Seasonings",
    "D·∫ßu ƒÉn": "Seasonings",
    "D·∫ßu h√†o, gi·∫•m, b∆°": "Seasonings",
    "Gia v·ªã n√™m s·∫µn": "Seasonings",
    "Mu·ªëi": "Seasonings",
    "H·∫°t n√™m, b·ªôt ng·ªçt, b·ªôt canh": "Seasonings",
    "Ti√™u, sa t·∫ø, ·ªõt b·ªôt": "Seasonings",
    "B·ªôt ngh·ªá, t·ªèi, h·ªìi, qu·∫ø,...": "Seasonings",
    "N∆∞·ªõc ch·∫•m, m·∫Øm": "Seasonings",
    "M·∫≠t ong, b·ªôt ngh·ªá": "Seasonings",

    # S·ªØa & c√°c s·∫£n ph·∫©m t·ª´ s·ªØa
    "S·ªØa t∆∞∆°i": "Milk",
    "S·ªØa ƒë·∫∑c": "Milk",
    "S·ªØa pha s·∫µn": "Milk",
    "S·ªØa h·∫°t, s·ªØa ƒë·∫≠u": "Milk",
    "S·ªØa ca cao, l√∫a m·∫°ch": "Milk",
    "S·ªØa tr√°i c√¢y, tr√† s·ªØa": "Milk",
    "S·ªØa chua ƒÉn": "Yogurt",
    "S·ªØa chua u·ªëng li·ªÅn": "Yogurt",
    "B∆° s·ªØa, ph√¥ mai": "Milk",

    # ƒê·ªì u·ªëng
    "Bia, n∆∞·ªõc c√≥ c·ªìn": "Alcoholic Beverages",
    "R∆∞·ª£u": "Alcoholic Beverages",
    "N∆∞·ªõc tr√†": "Beverages",
    "N∆∞·ªõc ng·ªçt": "Beverages",
    "N∆∞·ªõc √©p tr√°i c√¢y": "Beverages",
    "N∆∞·ªõc y·∫øn": "Beverages",
    "N∆∞·ªõc tƒÉng l·ª±c, b√π kho√°ng": "Beverages",
    "N∆∞·ªõc su·ªëi": "Beverages",
    "C√† ph√™ ho√† tan": "Beverages",
    "C√† ph√™ pha phin": "Beverages",
    "C√† ph√™ lon": "Beverages",
    "Tr√† kh√¥, t√∫i l·ªçc": "Beverages",

    # B√°nh k·∫πo, snack
    "B√°nh t∆∞∆°i, Sandwich": "Cakes",
    "B√°nh b√¥ng lan": "Cakes",
    "B√°nh quy": "Cakes",
    "B√°nh snack, rong bi·ªÉn": "Snacks",
    "B√°nh Chocopie": "Cakes",
    "B√°nh g·∫°o": "Cakes",
    "B√°nh qu·∫ø": "Cakes",
    "B√°nh que": "Cakes",
    "B√°nh x·ªëp": "Cakes",
    "K·∫πo c·ª©ng": "Candies",
    "K·∫πo d·∫ªo, k·∫πo marshmallow": "Candies",
    "K·∫πo singum": "Candies",
    "Socola": "Candies",
    "Tr√°i c√¢y s·∫•y": "Dried Fruits",
    "H·∫°t kh√¥": "Dried Fruits",
    "Rong bi·ªÉn c√°c lo·∫°i": "Snacks",
    "Rau c√¢u, th·∫°ch d·ª´a": "Fruit Jam",
    "M·ª©t tr√°i c√¢y": "Fruit Jam",
    "C∆°m ch√°y, b√°nh tr√°ng": "Snacks",

    # M√≥n ƒÉn ch·∫ø bi·∫øn s·∫µn, ƒë√¥ng l·∫°nh
    "L√†m s·∫µn, ƒÉn li·ªÅn": "Instant Foods",
    "S∆° ch·∫ø, t·∫©m ∆∞·ªõp": "Instant Foods",
    "N∆∞·ªõc l·∫©u, vi√™n th·∫£ l·∫©u": "Instant Foods",
    "Kim chi, ƒë·ªì chua": "Instant Foods",
    "Mandu, h√° c·∫£o, s·ªßi c·∫£o": "Instant Foods",
    "B√°nh bao, b√°nh m√¨, pizza": "Instant Foods",
    "Kem c√¢y, kem h·ªôp": "Ice Cream & Cheese",
    "B√°nh flan, th·∫°ch, ch√®": "Cakes",
    "Tr√°i c√¢y h·ªôp, siro": "Fruit Jam",

    # Kh√°c
    "C√° m·∫Øm, d∆∞a m·∫Øm": "Seasonings",
    "ƒê∆∞·ªùng": "Seasonings",
    "N∆∞·ªõc c·ªët d·ª´a lon": "Seasonings",
    "S·ªØa chua u·ªëng": "Yogurt",
    "Kh√¥ ch·∫ø bi·∫øn s·∫µn": "Instant Foods"
}

# ===== TEXT PROCESSING FUNCTIONS =====
def translate_vi2en(vi_text: str) -> str:
    # """Translate Vietnamese text to English"""
    try:
        inputs = tokenizer_vi2en(vi_text, return_tensors="pt")
        decoder_start_token_id = tokenizer_vi2en.lang_code_to_id["en_XX"]
        outputs = model_vi2en.generate(
            **inputs,
            decoder_start_token_id=decoder_start_token_id,
            num_beams=5,
            early_stopping=True
        )
        return tokenizer_vi2en.decode(outputs[0], skip_special_tokens=True)
    except Exception as e:
        print(f"Translation error for '{vi_text}': {e}")
        return ""

def tokenize_by_whitespace(text: str) -> List[str]:
    # """Tokenize text by whitespace, filter tokens >= 2 chars"""
    if text is None:
        return []
    return [token for token in text.lower().split() if len(token) >= 2]

def generate_ngrams(token: str, n: int) -> List[str]:
    # """Generate n-grams from a single token"""
    if token is None or len(token) < n:
        return []
    return [token[i:i+n] for i in range(len(token) - n + 1)]

def generate_token_ngrams(text: str, n: int) -> List[str]:
    # """Generate n-grams from all tokens in text"""
    tokens = tokenize_by_whitespace(text)
    ngrams = []
    for token in tokens:
        ngrams.extend(generate_ngrams(token, n))
    return ngrams

def parse_store_line(s: str) -> Dict[str, str]:
    # """Parse store location string into name and location"""
    name = s.split('(', 1)[0].strip()

    start = s.find('(')
    if start == -1:
        return {"store_name": name, "store_location": ""}
    
    level = 0
    end = None
    for i, ch in enumerate(s[start:], start):
        if ch == '(':
            level += 1
        elif ch == ')':
            level -= 1
            if level == 0:
                end = i
                break
    
    content = s[start+1:end] if end else ""
    location = re.sub(r'\([^)]*\)', '', content).strip()
    location = location.strip(',').strip()

    return {
        "store_name": name,
        "store_location": location
    }

# ===== PRICE & UNIT PROCESSING =====
def extract_net_value_and_unit_from_name(name: str, fallback_unit: str) -> tuple:
    """Extract net value and unit from product name"""
    tmp_name = name.lower()
    matches = re.findall(r"(\d+(?:\.\d+)?)\s*(g|ml|l√≠t|kg|g√≥i|l)\b", tmp_name)
    if matches:
        value, unit = matches[-1]  # use the LAST match
        return float(value), unit
    return 1, fallback_unit

def normalize_net_value(unit: str, net_value: float, name: str) -> tuple:
    """Normalize net value based on unit and product name"""
    unit = unit.lower()
    name_lower = name.lower()

    # Convert kg to g, l√≠t to ml
    if unit == "kg":
        return float(net_value) * 1000, "g"
    elif unit == "l√≠t":
        return float(net_value) * 1000, "ml"
    
    # Extract kg from name if unit is not standard
    if unit not in ["kg", "g", "ml", "l√≠t"]:
        match_kg = re.search(r"(\d+(\.\d+)?)\s*kg", name_lower)
        if match_kg:
            value = float(match_kg.group(1))
            return value * 1000, unit
    
    # Special case: t√∫i 1kg
    if unit == "t√∫i 1kg":
        return float(net_value) * 1000, "t√∫i"
    
    # T√∫i with fruit - assume 0.7kg
    if unit == "t√∫i" and "tr√°i" in name_lower:
        return 0.7 * 1000, unit

    # Box/tray with eggs count
    if unit in ["h·ªôp", "v·ªâ"] and "qu·∫£" in name_lower:
        matches = re.findall(rf"{unit}\s*(\d+)", name_lower)
        if matches:
            return sum(map(int, matches)), unit

    # Case/pack with multiple items
    match_pack = re.search(r"(th√πng|l·ªëc)\s*(\d+).*?(\d+(\.\d+)?)\s*(g|ml)", name_lower)
    if match_pack:
        count = int(match_pack.group(2))
        per_item = float(match_pack.group(3))
        return count * per_item, unit

    # Fallback: extract from name
    extracted_value, _ = extract_net_value_and_unit_from_name(name, unit)
    if extracted_value > 0:
        return extracted_value, unit

    return float(net_value) if net_value != 0 else 1, unit

def extract_best_price(product: dict) -> dict:
    """Extract best price information from product data"""
    base_price_info = product.get("productPrices", [])
    campaign_info = product.get("lstCampaingInfo", [])
    name = product.get("name", "")
    original_unit = product.get("unit", "").lower()

    def build_result(info: dict, unit: str, net_value: float):
        return {
            "name": name,
            "unit": unit, 
            "netUnitValue": net_value,
            "price": info.get("price"),
            "sysPrice": info.get("sysPrice"),
            "discountPercent": info.get("discountPercent"),
            "date_begin": info.get("startTime") or info.get("poDate"),
            "date_end": info.get("dueTime") or info.get("poDate"),
        }
    
    # Priority 1: Campaign pricing
    if campaign_info:
        campaign = campaign_info[0]
        campaign_price = campaign.get("productPrice", {})
        net_value = campaign_price.get("netUnitValue", 0)
        net_value, converted_unit = normalize_net_value(original_unit, net_value, name)
        return build_result(campaign_price, converted_unit, net_value)

    # Priority 2: Base pricing
    if base_price_info:
        price_info = base_price_info[0]
        net_value = price_info.get("netUnitValue", 0)
        net_value, converted_unit = normalize_net_value(original_unit, net_value, name)
        return build_result(price_info, converted_unit, net_value)

    # Fallback: No pricing info
    return {
        "name": name,
        "unit": original_unit,
        "netUnitValue": 1,
        "price": None,
        "sysPrice": None,
        "discountPercent": None,
        "date_begin": None,
        "date_end": None,
    }

# ===== PRODUCT DATA PROCESSING =====
def process_product_data(product: dict, category_name: str, store_id: int) -> dict:
    """Process raw product data - SKIP if already exists in DB"""
    from db import MongoDB
    db = MongoDB.get_db()
    
    sku = product.get("id")
    if not sku:
        raise ValueError("Product missing SKU")
    
    # Check if product already exists for this store
    coll_name = category_name.replace(" ", "_").replace("&", "and").replace(":", "").lower()
    existing = db[coll_name].find_one({"sku": sku, "store_id": store_id})
    
    if existing:
        print(f"‚è≠Ô∏è SKU {sku} already exists for store {store_id} - skipping processing")
        return None  # Skip to√†n b·ªô processing
    
    # Ch·ªâ process khi ch∆∞a t·ªìn t·∫°i
    print(f"üîÑ Processing new product: SKU {sku} for store {store_id}")
    
    # Translate name (ch·ªâ khi c·∫ßn thi·∫øt)
    english_name = translate_vi2en(product.get("name", ""))
    if not english_name:
        raise ValueError(f"Failed to translate name: {product.get('name', '')}")
    
    # Extract price information
    price_info = extract_best_price(product)
    
    # Generate search tokens
    token_ngrams = generate_token_ngrams(english_name, 2)
    
    # Build standardized product data
    return {
        "sku": sku,
        "name": product["name"],
        "name_en": english_name,
        "unit": price_info["unit"].lower(),
        "netUnitValue": price_info["netUnitValue"], 
        "token_ngrams": token_ngrams,
        "category": category_name, 
        "store_id": store_id, 
        "url": f"https://www.bachhoaxanh.com{product['url']}",
        "image": product["avatar"],
        "promotion": product.get("promotionText", ""),
        "price": price_info["price"],
        "sysPrice": price_info["sysPrice"],
        "discountPercent": price_info["discountPercent"],
        "date_begin": price_info["date_begin"],
        "date_end": price_info["date_end"],
        "crawled_at": datetime.utcnow().isoformat(),
    }

# ===== DATABASE OPERATIONS =====
async def upsert_product(product_data: dict, category_title: str, db):
    """Upsert single product to MongoDB"""
    coll_name = category_title.replace(" ", "_").lower()
    product_db = db[coll_name]

    sku = product_data.get("sku")
    if not sku:
        print("No SKU found for product, skipping upsert.")
        return

    product_db.update_one(
        {"sku": sku},
        {"$set": product_data},
        upsert=True
    )
    print(f"‚úì Upserted product: {product_data.get('name', '')} "
          f"(SKU: {sku}) into collection: {coll_name}")

async def upsert_products_bulk(product_list: List[dict], category_title: str, db):
    """Bulk upsert products to MongoDB"""
    coll_name = category_title.replace(" ", "_").lower()
    collection = db[coll_name]
    ops = []
    
    for p in product_list:
        sku = p.get("sku")
        if not sku:
            continue
        ops.append(
            UpdateOne(
                {"sku": sku},
                {"$set": p},
                upsert=True
            )
        )
    
    if not ops:
        return
    
    result = collection.bulk_write(ops, ordered=False)
    print(f"‚úì Bulk upserted {len(ops)} products into `{coll_name}` "
          f"(upserted: {result.upserted_count}, modified: {result.modified_count})")

def reset_category_collections(db):
    """Drop all category collections from database"""
    try:
        for cat_doc in db.categorys.find({}, {"name": 1}):
            coll_name = cat_doc["name"].lower().replace(" ", "_")
            if coll_name in db.list_collection_names():
                print(f"Dropping collection: {coll_name}")
                db.drop_collection(coll_name)
    except Exception as e:
        print(f"Error resetting collections: {e}")
        return